{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW, Adam\n",
    "from utils.utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mateu/.cache\\torch\\hub\\mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(UNet, self).__init__()\n",
    "        self.model = base_model\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.fc1 = nn.Linear(28900, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.output = nn.Linear(128, 3) \n",
    "\n",
    "    def forward(self, x):\n",
    "        segmented = self.model(x)\n",
    "        concatenated = torch.cat((x, segmented), dim=1)\n",
    "        classified = self.maxpool(concatenated)\n",
    "        classified = classified.view(classified.size(0), -1)\n",
    "        classified = self.fc1(classified)\n",
    "        classified = self.relu1(classified)\n",
    "        classified = self.fc2(classified)\n",
    "        classified = self.relu2(classified)\n",
    "        classified = self.fc3(classified)\n",
    "        classified = self.relu3(classified)\n",
    "        output = self.output(classified)\n",
    "        return segmented, output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# new cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Parts of the U-Net model \"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, n_channels, n_classes, bilinear=False):\n",
    "        super(UNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.bilinear = bilinear\n",
    "\n",
    "        self.inc = (DoubleConv(n_channels, 64))\n",
    "        self.down1 = (Down(64, 128))\n",
    "        self.down2 = (Down(128, 256))\n",
    "        self.down3 = (Down(256, 512))\n",
    "        factor = 2 if bilinear else 1\n",
    "        self.down4 = (Down(512, 1024 // factor))\n",
    "        self.up1 = (Up(1024, 512 // factor, bilinear))\n",
    "        self.up2 = (Up(512, 256 // factor, bilinear))\n",
    "        self.up3 = (Up(256, 128 // factor, bilinear))\n",
    "        self.up4 = (Up(128, 64, bilinear))\n",
    "        self.outc = (OutConv(64, n_classes))\n",
    "        self.simoid = nn.Sigmoid()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(1024, 512, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(512, 256, kernel_size=3)\n",
    "        self.conv3 = nn.Conv2d(256, 128, kernel_size=3)\n",
    "        self.max_pool = nn.MaxPool2d(3)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear1 = nn.Linear(1152, 256)\n",
    "        self.linear2 = nn.Linear(256, 64)\n",
    "        self.linear3 = nn.Linear(64, 16)\n",
    "        self.linear4 = nn.Linear(16, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        logits = self.simoid(logits)\n",
    "\n",
    "        x_class = self.conv1(x5)\n",
    "        x_class = self.conv2(x_class)\n",
    "        x_class = self.conv3(x_class)\n",
    "        x_class = self.max_pool(x_class)\n",
    "        x_class = self.flatten(x_class)\n",
    "        x_class = torch.relu(self.linear1(x_class))\n",
    "        x_class = torch.relu(self.linear2(x_class))\n",
    "        x_class = torch.relu(self.linear3(x_class))\n",
    "        x_class = self.linear4(x_class)\n",
    "        return logits, x_class\n",
    "\n",
    "    def use_checkpointing(self):\n",
    "        self.inc = torch.utils.checkpoint(self.inc)\n",
    "        self.down1 = torch.utils.checkpoint(self.down1)\n",
    "        self.down2 = torch.utils.checkpoint(self.down2)\n",
    "        self.down3 = torch.utils.checkpoint(self.down3)\n",
    "        self.down4 = torch.utils.checkpoint(self.down4)\n",
    "        self.up1 = torch.utils.checkpoint(self.up1)\n",
    "        self.up2 = torch.utils.checkpoint(self.up2)\n",
    "        self.up3 = torch.utils.checkpoint(self.up3)\n",
    "        self.up4 = torch.utils.checkpoint(self.up4)\n",
    "        self.outc = torch.utils.checkpoint(self.outc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (inc): DoubleConv(\n",
      "    (double_conv): Sequential(\n",
      "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU(inplace=True)\n",
      "      (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (5): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (down1): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down2): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down3): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (down4): Down(\n",
      "    (maxpool_conv): Sequential(\n",
      "      (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "      (1): DoubleConv(\n",
      "        (double_conv): Sequential(\n",
      "          (0): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (2): ReLU(inplace=True)\n",
      "          (3): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (4): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (5): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up1): Up(\n",
      "    (up): ConvTranspose2d(1024, 512, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up2): Up(\n",
      "    (up): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up3): Up(\n",
      "    (up): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (up4): Up(\n",
      "    (up): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (conv): DoubleConv(\n",
      "      (double_conv): Sequential(\n",
      "        (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace=True)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (outc): OutConv(\n",
      "    (conv): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (simoid): Sigmoid()\n",
      "  (conv1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv3): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (max_pool): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear1): Linear(in_features=1152, out_features=256, bias=True)\n",
      "  (linear2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (linear3): Linear(in_features=64, out_features=16, bias=True)\n",
      "  (linear4): Linear(in_features=16, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "unet = UNet(3, 1)\n",
    "print(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters:  37544388\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "total_params = count_parameters(unet)\n",
    "print(\"Total trainable parameters: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_images, benign_masks = read_data(directory = \"data//benign\", target_size=(256, 256))\n",
    "normal_images, normal_masks = read_data(directory = \"data//normal\", target_size=(256, 256))\n",
    "malignant_images, malignant_masks = read_data(directory = \"data//malignant\", target_size=(256, 256))\n",
    "\n",
    "benign_masks = np.reshape(benign_masks, (benign_masks.shape[0], 1, benign_masks.shape[1], benign_masks.shape[2]))\n",
    "normal_masks = np.reshape(normal_masks, (normal_masks.shape[0], 1, normal_masks.shape[1], normal_masks.shape[2]))\n",
    "malignant_masks = np.reshape(malignant_masks, (malignant_masks.shape[0], 1, malignant_masks.shape[1], malignant_masks.shape[2]))\n",
    "\n",
    "benign_images = np.transpose(benign_images, (0, 3, 1, 2))\n",
    "normal_images = np.transpose(normal_images, (0, 3, 1, 2))\n",
    "malignant_images = np.transpose(malignant_images, (0, 3, 1, 2))\n",
    "\n",
    "benign_values = np.array([1, 0, 0], dtype = np.float32)\n",
    "normal_values = np.array([0, 1, 0], dtype = np.float32)\n",
    "malignant_values = np.array([0, 0, 1], dtype = np.float32)\n",
    "num_benign = len(benign_images)\n",
    "num_normal = len(normal_images)\n",
    "num_malignant = len(malignant_images)\n",
    "y_classification = []\n",
    "y_segmentation = []\n",
    "X = []\n",
    "\n",
    "for i in range(num_benign):\n",
    "    y_classification.append(benign_values)\n",
    "for i in range(num_normal):\n",
    "    y_classification.append(normal_values)\n",
    "for i in range(num_malignant):\n",
    "    y_classification.append(malignant_values)\n",
    "y_classification = np.array(y_classification, dtype = np.float32)\n",
    "\n",
    "benign_masks[benign_masks>0] = 1\n",
    "normal_masks[normal_masks>0] = 1\n",
    "malignant_masks[malignant_masks>0] = 1\n",
    "benign_images = benign_images.astype(dtype = np.float32)\n",
    "normal_images = normal_images.astype(dtype = np.float32)\n",
    "malignant_images = malignant_images.astype(dtype = np.float32)\n",
    "benign_images /= 255\n",
    "normal_images /= 255\n",
    "malignant_images /= 255\n",
    "\n",
    "for img in benign_images:\n",
    "    X.append(img)\n",
    "for img in normal_images:\n",
    "    X.append(img)\n",
    "for img in malignant_images:\n",
    "    X.append(img)\n",
    "X = np.array(X, dtype=np.float32)\n",
    "\n",
    "for img in benign_masks:\n",
    "    y_segmentation.append(img)\n",
    "for img in normal_masks:\n",
    "    y_segmentation.append(img)\n",
    "for img in malignant_masks:\n",
    "    y_segmentation.append(img)\n",
    "y_segmentation = np.array(y_segmentation, dtype=np.float32)\n",
    "\n",
    "num_samples = len(y_segmentation)\n",
    "perm = np.random.permutation(num_samples)\n",
    "\n",
    "X = X[perm]\n",
    "y_classification = y_classification[perm]\n",
    "y_segmentation = y_segmentation[perm]\n",
    "\n",
    "total_samples = len(X)\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.2 * total_samples)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_classification_train = y_classification[:train_size]\n",
    "y_segmentation_train = y_segmentation[:train_size]\n",
    "\n",
    "X_val = X[train_size:train_size + val_size]\n",
    "y_classification_val = y_classification[train_size:train_size + val_size]\n",
    "y_segmentation_val = y_segmentation[train_size:train_size + val_size]\n",
    "\n",
    "X_test = X[train_size + val_size:]\n",
    "y_classification_test = y_classification[train_size + val_size:]\n",
    "y_segmentation_test = y_segmentation[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y1, y2):\n",
    "        self.X = X\n",
    "        self.y1 = y1\n",
    "        self.y2 = y2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y1[idx], self.y2[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train)\n",
    "y_classification_train = torch.Tensor(y_classification_train)\n",
    "y_segmentation_train = torch.Tensor(y_segmentation_train)\n",
    "\n",
    "X_val = torch.Tensor(X_val)\n",
    "y_classification_val = torch.Tensor(y_classification_val)\n",
    "y_segmentation_val = torch.Tensor(y_segmentation_val)\n",
    "\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_classification_test = torch.Tensor(y_classification_test)\n",
    "y_segmentation_test = torch.Tensor(y_segmentation_test)\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_classification_train, y_segmentation_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_dataset = CustomDataset(X_val, y_classification_val, y_segmentation_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = CustomDataset(X_test, y_classification_test, y_segmentation_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_segmentation_train.shape\n",
    "y_classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Classification Training Loss: 1.0129894052233015, Train accuracy: 0.4928571428571429, Segmentation Training Loss: 0.985618497644152, Classification Validation Loss: 2.6705032706260683, Segmentation Validation Loss: 0.8495967745780945, Classification Test Loss: 5.451157522201538, Segmentation Test Loss: 0.838263475894928\n"
     ]
    }
   ],
   "source": [
    "unet = UNet(3, 1)\n",
    "unet.train()\n",
    "\n",
    "optimizer = Adam(unet.parameters(), lr = 1e-4)\n",
    "segmentation_loss = torch.nn.BCEWithLogitsLoss()\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "classification_loss_train = []\n",
    "segmentation_loss_train = []\n",
    "classification_loss_val = []\n",
    "segmentation_loss_val = []\n",
    "classification_loss_test = []\n",
    "segmentation_loss_test = []\n",
    "accuracy_list = []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    unet.train()\n",
    "    train_loss_class = []\n",
    "    train_loss_seg = []\n",
    "    val_loss_class = []\n",
    "    val_loss_seg = []\n",
    "    test_loss_class = []\n",
    "    test_loss_seg = []\n",
    "    accuracy = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X_data, y_class, y_seg = batch\n",
    "        optimizer.zero_grad()\n",
    "        segmentation_output, classification_output = unet(X_data)\n",
    "        segmentation_output = torch.sigmoid(segmentation_output)\n",
    "        classification_output = torch.softmax(classification_output, dim=1)\n",
    "        seg_loss = segmentation_loss(segmentation_output, y_seg)\n",
    "        class_loss = classification_loss(classification_output, y_class)\n",
    "        loss = seg_loss + class_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_class.append(class_loss.item())\n",
    "        train_loss_seg.append(seg_loss.item())\n",
    "        predicted_classes = torch.argmax(classification_output, dim=1)\n",
    "        temp_class = torch.argmax(y_class, dim=1)\n",
    "        accuracy.append((predicted_classes == temp_class).float().mean().item())\n",
    "\n",
    "    accuracy = sum(accuracy)/len(accuracy)\n",
    "    accuracy_list.append(accuracy)\n",
    "    classification_loss_train.append(sum(train_loss_class) / len(train_loss_class))\n",
    "    segmentation_loss_train.append(sum(train_loss_seg) / len(train_loss_seg))\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_class = []\n",
    "        val_loss_seg = []\n",
    "        for batch in val_loader:\n",
    "            X_val, y_class_val, y_seg_val = batch\n",
    "            segmentation_output_val, classification_output_val = unet(X_val)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_val = segmentation_loss(segmentation_output_val, y_seg_val)\n",
    "            class_loss_val = classification_loss(classification_output_val, y_class_val)\n",
    "            val_loss_class.append(class_loss_val.item())\n",
    "            val_loss_seg.append(seg_loss_val.item())\n",
    "        \n",
    "        classification_loss_val.append(sum(val_loss_class) / len(val_loss_class))\n",
    "        segmentation_loss_val.append(sum(val_loss_seg) / len(val_loss_seg))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss_class = []\n",
    "        test_loss_seg = []\n",
    "        for batch in test_loader:\n",
    "            X_test, y_class_test, y_seg_test = batch\n",
    "            segmentation_output_test, classification_output_test = unet(X_test)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_test = segmentation_loss(segmentation_output_test, y_seg_test)\n",
    "            class_loss_test = classification_loss(classification_output_test, y_class_test)\n",
    "            test_loss_class.append(class_loss_test.item())\n",
    "            test_loss_seg.append(seg_loss_test.item())\n",
    "        \n",
    "        classification_loss_test.append(sum(test_loss_class) / len(test_loss_class))\n",
    "        segmentation_loss_test.append(sum(test_loss_seg) / len(test_loss_seg))\n",
    "\n",
    "    print(f\"Epoch: {i}, Classification Training Loss: {classification_loss_train[-1]}, Train accuracy: {accuracy_list[-1]}, Segmentation Training Loss: {segmentation_loss_train[-1]}, Classification Validation Loss: {classification_loss_val[-1]}, Segmentation Validation Loss: {segmentation_loss_val[-1]}, Classification Test Loss: {classification_loss_test[-1]}, Segmentation Test Loss: {segmentation_loss_test[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet.state_dict(), 'unet1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_to_save = {\n",
    "    'classification_loss_train': classification_loss_train,\n",
    "    'segmentation_loss_train': segmentation_loss_train,\n",
    "    'classification_loss_val': classification_loss_val,\n",
    "    'segmentation_loss_val': segmentation_loss_val,\n",
    "    'classification_loss_test': classification_loss_test,\n",
    "    'segmentation_loss_test': segmentation_loss_test\n",
    "}\n",
    "\n",
    "with open('unet_results_1.pkl', 'wb') as f:\n",
    "    pickle.dump(lists_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(model)\n",
    "unet.train()\n",
    "\n",
    "optimizer = Adam(unet.parameters(), lr = 1e-4)\n",
    "segmentation_loss = torch.nn.MSELoss()\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "classification_loss_train = []\n",
    "segmentation_loss_train = []\n",
    "classification_loss_val = []\n",
    "segmentation_loss_val = []\n",
    "classification_loss_test = []\n",
    "segmentation_loss_test = []\n",
    "accuracy_list = []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    unet.train()\n",
    "    train_loss_class = []\n",
    "    train_loss_seg = []\n",
    "    val_loss_class = []\n",
    "    val_loss_seg = []\n",
    "    test_loss_class = []\n",
    "    test_loss_seg = []\n",
    "    accuracy = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X_data, y_class, y_seg = batch\n",
    "        optimizer.zero_grad()\n",
    "        segmentation_output, _ = unet(X_data)\n",
    "        segmentation_output = torch.sigmoid(segmentation_output)\n",
    "        seg_loss = segmentation_loss(segmentation_output, y_seg)\n",
    "        seg_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        _,classification_output  = unet(X_data)\n",
    "        classification_output = torch.softmax(classification_output, dim=1)\n",
    "        class_loss = classification_loss(classification_output, y_class)\n",
    "        class_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_class.append(class_loss.item())\n",
    "        train_loss_seg.append(seg_loss.item())\n",
    "        temp_class = torch.argmax(y_class, dim=1)\n",
    "        accuracy.append((predicted_classes == temp_class).float().mean().item())\n",
    "\n",
    "    accuracy = sum(accuracy)/len(accuracy)\n",
    "    accuracy_list.append(accuracy)\n",
    "    classification_loss_train.append(sum(train_loss_class) / len(train_loss_class))\n",
    "    segmentation_loss_train.append(sum(train_loss_seg) / len(train_loss_seg))\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_class = []\n",
    "        val_loss_seg = []\n",
    "        for batch in val_loader:\n",
    "            X_val, y_class_val, y_seg_val = batch\n",
    "            segmentation_output_val, classification_output_val = unet(X_val)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_val = segmentation_loss(segmentation_output_val, y_seg_val)\n",
    "            class_loss_val = classification_loss(classification_output_val, y_class_val)\n",
    "            val_loss_class.append(class_loss_val.item())\n",
    "            val_loss_seg.append(seg_loss_val.item())\n",
    "        \n",
    "        classification_loss_val.append(sum(val_loss_class) / len(val_loss_class))\n",
    "        segmentation_loss_val.append(sum(val_loss_seg) / len(val_loss_seg))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss_class = []\n",
    "        test_loss_seg = []\n",
    "        for batch in test_loader:\n",
    "            X_test, y_class_test, y_seg_test = batch\n",
    "            segmentation_output_test, classification_output_test = unet(X_test)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_test = segmentation_loss(segmentation_output_test, y_seg_test)\n",
    "            class_loss_test = classification_loss(classification_output_test, y_class_test)\n",
    "            test_loss_class.append(class_loss_test.item())\n",
    "            test_loss_seg.append(seg_loss_test.item())\n",
    "        \n",
    "        classification_loss_test.append(sum(test_loss_class) / len(test_loss_class))\n",
    "        segmentation_loss_test.append(sum(test_loss_seg) / len(test_loss_seg))\n",
    "\n",
    "    print(f\"Epoch: {i}, Classification Training Loss: {classification_loss_train[-1]}, Train accuracy: {accuracy_list[-1]}, Segmentation Training Loss: {segmentation_loss_train[-1]}, Classification Validation Loss: {classification_loss_val[-1]}, Segmentation Validation Loss: {segmentation_loss_val[-1]}, Classification Test Loss: {classification_loss_test[-1]}, Segmentation Test Loss: {segmentation_loss_test[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet.state_dict(), 'unet2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_to_save = {\n",
    "    'classification_loss_train': classification_loss_train,\n",
    "    'segmentation_loss_train': segmentation_loss_train,\n",
    "    'classification_loss_val': classification_loss_val,\n",
    "    'segmentation_loss_val': segmentation_loss_val,\n",
    "    'classification_loss_test': classification_loss_test,\n",
    "    'segmentation_loss_test': segmentation_loss_test\n",
    "}\n",
    "\n",
    "with open('unet_results_2.pkl', 'wb') as f:\n",
    "    pickle.dump(lists_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = 0.8\n",
    "b2 = 0.85\n",
    "unet = UNet(model)\n",
    "unet.train()\n",
    "\n",
    "optimizer = Adam(unet.parameters(), lr = 1e-3, betas=(b1, b2), weight_decay=0.0005)\n",
    "segmentation_loss = torch.nn.MSELoss()\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "classification_loss_train = []\n",
    "segmentation_loss_train = []\n",
    "classification_loss_val = []\n",
    "segmentation_loss_val = []\n",
    "classification_loss_test = []\n",
    "segmentation_loss_test = []\n",
    "accuracy_list = []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    unet.train()\n",
    "    train_loss_class = []\n",
    "    train_loss_seg = []\n",
    "    val_loss_class = []\n",
    "    val_loss_seg = []\n",
    "    test_loss_class = []\n",
    "    test_loss_seg = []\n",
    "    accuracy = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X_data, y_class, y_seg = batch\n",
    "        optimizer.zero_grad()\n",
    "        segmentation_output, classification_output = unet(X_data)\n",
    "        segmentation_output = torch.sigmoid(segmentation_output)\n",
    "        classification_output = torch.softmax(classification_output, dim=1)\n",
    "        seg_loss = segmentation_loss(segmentation_output, y_seg)\n",
    "        class_loss = classification_loss(classification_output, y_class)\n",
    "        loss = seg_loss + class_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_class.append(class_loss.item())\n",
    "        train_loss_seg.append(seg_loss.item())\n",
    "        temp_class = torch.argmax(y_class, dim=1)\n",
    "        accuracy.append((predicted_classes == temp_class).float().mean().item())\n",
    "\n",
    "    accuracy = sum(accuracy)/len(accuracy)\n",
    "    accuracy_list.append(accuracy)\n",
    "    classification_loss_train.append(sum(train_loss_class) / len(train_loss_class))\n",
    "    segmentation_loss_train.append(sum(train_loss_seg) / len(train_loss_seg))\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_class = []\n",
    "        val_loss_seg = []\n",
    "        for batch in val_loader:\n",
    "            X_val, y_class_val, y_seg_val = batch\n",
    "            segmentation_output_val, classification_output_val = unet(X_val)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_val = segmentation_loss(segmentation_output_val, y_seg_val)\n",
    "            class_loss_val = classification_loss(classification_output_val, y_class_val)\n",
    "            val_loss_class.append(class_loss_val.item())\n",
    "            val_loss_seg.append(seg_loss_val.item())\n",
    "        \n",
    "        classification_loss_val.append(sum(val_loss_class) / len(val_loss_class))\n",
    "        segmentation_loss_val.append(sum(val_loss_seg) / len(val_loss_seg))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss_class = []\n",
    "        test_loss_seg = []\n",
    "        for batch in test_loader:\n",
    "            X_test, y_class_test, y_seg_test = batch\n",
    "            segmentation_output_test, classification_output_test = unet(X_test)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_test = segmentation_loss(segmentation_output_test, y_seg_test)\n",
    "            class_loss_test = classification_loss(classification_output_test, y_class_test)\n",
    "            test_loss_class.append(class_loss_test.item())\n",
    "            test_loss_seg.append(seg_loss_test.item())\n",
    "        \n",
    "        classification_loss_test.append(sum(test_loss_class) / len(test_loss_class))\n",
    "        segmentation_loss_test.append(sum(test_loss_seg) / len(test_loss_seg))\n",
    "\n",
    "    print(f\"Epoch: {i}, Classification Training Loss: {classification_loss_train[-1]}, Train accuracy: {accuracy_list[-1]}, Segmentation Training Loss: {segmentation_loss_train[-1]}, Classification Validation Loss: {classification_loss_val[-1]}, Segmentation Validation Loss: {segmentation_loss_val[-1]}, Classification Test Loss: {classification_loss_test[-1]}, Segmentation Test Loss: {segmentation_loss_test[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet.state_dict(), 'unet3.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_to_save = {\n",
    "    'classification_loss_train': classification_loss_train,\n",
    "    'segmentation_loss_train': segmentation_loss_train,\n",
    "    'classification_loss_val': classification_loss_val,\n",
    "    'segmentation_loss_val': segmentation_loss_val,\n",
    "    'classification_loss_test': classification_loss_test,\n",
    "    'segmentation_loss_test': segmentation_loss_test\n",
    "}\n",
    "\n",
    "with open('unet_results_3.pkl', 'wb') as f:\n",
    "    pickle.dump(lists_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(model)\n",
    "unet.train()\n",
    "\n",
    "optimizer = AdamW(unet.parameters(), lr = 1e-4)\n",
    "segmentation_loss = torch.nn.MSELoss()\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "classification_loss_train = []\n",
    "segmentation_loss_train = []\n",
    "classification_loss_val = []\n",
    "segmentation_loss_val = []\n",
    "classification_loss_test = []\n",
    "segmentation_loss_test = []\n",
    "accuracy_list = []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    unet.train()\n",
    "    train_loss_class = []\n",
    "    train_loss_seg = []\n",
    "    val_loss_class = []\n",
    "    val_loss_seg = []\n",
    "    test_loss_class = []\n",
    "    test_loss_seg = []\n",
    "    accuracy = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X_data, y_class, y_seg = batch\n",
    "        optimizer.zero_grad()\n",
    "        segmentation_output, classification_output = unet(X_data)\n",
    "        segmentation_output = torch.sigmoid(segmentation_output)\n",
    "        classification_output = torch.softmax(classification_output, dim=1)\n",
    "        seg_loss = segmentation_loss(segmentation_output, y_seg)\n",
    "        class_loss = classification_loss(classification_output, y_class)\n",
    "        loss = seg_loss + class_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_class.append(class_loss.item())\n",
    "        train_loss_seg.append(seg_loss.item())\n",
    "        temp_class = torch.argmax(y_class, dim=1)\n",
    "        accuracy.append((predicted_classes == temp_class).float().mean().item())\n",
    "\n",
    "    accuracy = sum(accuracy)/len(accuracy)\n",
    "    accuracy_list.append(accuracy)\n",
    "    classification_loss_train.append(sum(train_loss_class) / len(train_loss_class))\n",
    "    segmentation_loss_train.append(sum(train_loss_seg) / len(train_loss_seg))\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_class = []\n",
    "        val_loss_seg = []\n",
    "        for batch in val_loader:\n",
    "            X_val, y_class_val, y_seg_val = batch\n",
    "            segmentation_output_val, classification_output_val = unet(X_val)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_val = segmentation_loss(segmentation_output_val, y_seg_val)\n",
    "            class_loss_val = classification_loss(classification_output_val, y_class_val)\n",
    "            val_loss_class.append(class_loss_val.item())\n",
    "            val_loss_seg.append(seg_loss_val.item())\n",
    "        \n",
    "        classification_loss_val.append(sum(val_loss_class) / len(val_loss_class))\n",
    "        segmentation_loss_val.append(sum(val_loss_seg) / len(val_loss_seg))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss_class = []\n",
    "        test_loss_seg = []\n",
    "        for batch in test_loader:\n",
    "            X_test, y_class_test, y_seg_test = batch\n",
    "            segmentation_output_test, classification_output_test = unet(X_test)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_test = segmentation_loss(segmentation_output_test, y_seg_test)\n",
    "            class_loss_test = classification_loss(classification_output_test, y_class_test)\n",
    "            test_loss_class.append(class_loss_test.item())\n",
    "            test_loss_seg.append(seg_loss_test.item())\n",
    "        \n",
    "        classification_loss_test.append(sum(test_loss_class) / len(test_loss_class))\n",
    "        segmentation_loss_test.append(sum(test_loss_seg) / len(test_loss_seg))\n",
    "\n",
    "    print(f\"Epoch: {i}, Classification Training Loss: {classification_loss_train[-1]}, Train accuracy: {accuracy_list[-1]}, Segmentation Training Loss: {segmentation_loss_train[-1]}, Classification Validation Loss: {classification_loss_val[-1]}, Segmentation Validation Loss: {segmentation_loss_val[-1]}, Classification Test Loss: {classification_loss_test[-1]}, Segmentation Test Loss: {segmentation_loss_test[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(unet.state_dict(), 'unet4.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists_to_save = {\n",
    "    'classification_loss_train': classification_loss_train,\n",
    "    'segmentation_loss_train': segmentation_loss_train,\n",
    "    'classification_loss_val': classification_loss_val,\n",
    "    'segmentation_loss_val': segmentation_loss_val,\n",
    "    'classification_loss_test': classification_loss_test,\n",
    "    'segmentation_loss_test': segmentation_loss_test\n",
    "}\n",
    "\n",
    "with open('unet_results_4.pkl', 'wb') as f:\n",
    "    pickle.dump(lists_to_save, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
