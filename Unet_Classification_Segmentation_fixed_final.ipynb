{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import cv2\n",
        "\n",
        "def read_data(directory, target_size = (300, 300)):\n",
        "    all_files = os.listdir(directory)\n",
        "    not_mask_images = sorted([file for file in all_files if \"mask\" not in file])\n",
        "    mask_images = sorted([file for file in all_files if \"mask\" in file])\n",
        "    images = []\n",
        "    masks = []\n",
        "    temp_masks = []\n",
        "\n",
        "    for file_name in not_mask_images:\n",
        "        image = cv2.imread(os.path.join(directory, file_name))\n",
        "        images.append(cv2.resize(image, target_size))\n",
        "\n",
        "    prev_file = None\n",
        "    for file_name in mask_images:\n",
        "        image = cv2.imread(os.path.join(directory, file_name), cv2.IMREAD_GRAYSCALE)\n",
        "        image = cv2.resize(image, target_size)\n",
        "        if prev_file != None and (str(prev_file).replace(\".png\", \"\").replace(\" \", \"_\") in str(file_name).replace(\".png\", \"\").replace(\" \",\"_\")):\n",
        "            masks[-1] += image\n",
        "        else:\n",
        "            temp_masks.append(file_name)\n",
        "            masks.append(image)\n",
        "            prev_file = file_name\n",
        "\n",
        "    return np.array(images), np.array(masks)\n",
        "\n",
        "def show_image(data, index):\n",
        "    image = data[index]\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kzZ7zFrgxTsV"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "E0Q-1pjEqShN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW, Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oz0TDEYXrNKn",
        "outputId": "9f107f40-04d8-42d7-f233-5ce5c231c6cf"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QMVTpzobqShS",
        "outputId": "df5de0c8-d0c5-43e3-b1b5-ab2bbd24798c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/mateuszbuda_brain-segmentation-pytorch_master\n"
          ]
        }
      ],
      "source": [
        "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
        "    in_channels=3, out_channels=1, init_features=32, pretrained=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "n0SRtJPy7wyC"
      },
      "execution_count": 166,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "id": "d94KuqdoqShT"
      },
      "outputs": [],
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(UNet, self).__init__()\n",
        "        self.model = base_model\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
        "        self.fc1 = nn.Linear(28900, 512)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.output = nn.Linear(128, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        segmented = self.model(x)\n",
        "        concatenated = torch.cat((x, segmented), dim=1)\n",
        "        classified = self.maxpool(concatenated)\n",
        "        classified = classified.view(classified.size(0), -1)\n",
        "        classified = self.fc1(classified)\n",
        "        classified = self.relu1(classified)\n",
        "        classified = self.fc2(classified)\n",
        "        classified = self.relu2(classified)\n",
        "        classified = self.fc3(classified)\n",
        "        classified = self.relu3(classified)\n",
        "        output = self.output(classified)\n",
        "        return segmented, output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kelmydo3qShU",
        "outputId": "5a2abbfe-a001-41d4-e90b-420c2813c167"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "UNet(\n",
            "  (model): UNet(\n",
            "    (encoder1): Sequential(\n",
            "      (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (enc1relu1): ReLU(inplace=True)\n",
            "      (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (enc1relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (encoder2): Sequential(\n",
            "      (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (enc2relu1): ReLU(inplace=True)\n",
            "      (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (enc2relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (encoder3): Sequential(\n",
            "      (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (enc3relu1): ReLU(inplace=True)\n",
            "      (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (enc3relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (encoder4): Sequential(\n",
            "      (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (enc4relu1): ReLU(inplace=True)\n",
            "      (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (enc4relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (bottleneck): Sequential(\n",
            "      (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bottleneckrelu1): ReLU(inplace=True)\n",
            "      (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (bottleneckrelu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (decoder4): Sequential(\n",
            "      (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dec4relu1): ReLU(inplace=True)\n",
            "      (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dec4relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (decoder3): Sequential(\n",
            "      (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dec3relu1): ReLU(inplace=True)\n",
            "      (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dec3relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (decoder2): Sequential(\n",
            "      (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dec2relu1): ReLU(inplace=True)\n",
            "      (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dec2relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
            "    (decoder1): Sequential(\n",
            "      (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dec1relu1): ReLU(inplace=True)\n",
            "      (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (dec1relu2): ReLU(inplace=True)\n",
            "    )\n",
            "    (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
            "  )\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
            "  (fc1): Linear(in_features=28900, out_features=512, bias=True)\n",
            "  (relu1): ReLU()\n",
            "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (relu2): ReLU()\n",
            "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (relu3): ReLU()\n",
            "  (output): Linear(in_features=128, out_features=3, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "unet = UNet(model).to(device)\n",
        "print(unet)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUwo-i5zqShV",
        "outputId": "e70c6973-b938-4adf-bd67-42cbc246e1d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters:  22724964\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# Example usage:\n",
        "total_params = count_parameters(unet)\n",
        "print(\"Total trainable parameters: \", total_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "hB_bxU79qShV"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "_LjToHDNqShW"
      },
      "outputs": [],
      "source": [
        "benign_images, benign_masks = read_data(directory = \"/content/drive/MyDrive/data/benign\", target_size=(256, 256))\n",
        "normal_images, normal_masks = read_data(directory = \"/content/drive/MyDrive/data/normal\", target_size=(256, 256))\n",
        "malignant_images, malignant_masks = read_data(directory = \"/content/drive/MyDrive/data/malignant\", target_size=(256, 256))\n",
        "\n",
        "benign_masks = np.reshape(benign_masks, (benign_masks.shape[0], 1, benign_masks.shape[1], benign_masks.shape[2]))\n",
        "normal_masks = np.reshape(normal_masks, (normal_masks.shape[0], 1, normal_masks.shape[1], normal_masks.shape[2]))\n",
        "malignant_masks = np.reshape(malignant_masks, (malignant_masks.shape[0], 1, malignant_masks.shape[1], malignant_masks.shape[2]))\n",
        "\n",
        "benign_images = np.transpose(benign_images, (0, 3, 1, 2))\n",
        "normal_images = np.transpose(normal_images, (0, 3, 1, 2))\n",
        "malignant_images = np.transpose(malignant_images, (0, 3, 1, 2))\n",
        "\n",
        "benign_values = np.array([1, 0, 0], dtype = np.float32)\n",
        "normal_values = np.array([0, 1, 0], dtype = np.float32)\n",
        "malignant_values = np.array([0, 0, 1], dtype = np.float32)\n",
        "num_benign = len(benign_images)\n",
        "num_normal = len(normal_images)\n",
        "num_malignant = len(malignant_images)\n",
        "y_classification = []\n",
        "y_segmentation = []\n",
        "X = []\n",
        "\n",
        "for i in range(num_benign):\n",
        "    y_classification.append(benign_values)\n",
        "for i in range(num_normal):\n",
        "    y_classification.append(normal_values)\n",
        "for i in range(num_malignant):\n",
        "    y_classification.append(malignant_values)\n",
        "y_classification = np.array(y_classification, dtype = np.float32)\n",
        "\n",
        "benign_masks[benign_masks>0] = 1\n",
        "normal_masks[normal_masks>0] = 1\n",
        "malignant_masks[malignant_masks>0] = 1\n",
        "benign_images = benign_images.astype(dtype = np.float32)\n",
        "normal_images = normal_images.astype(dtype = np.float32)\n",
        "malignant_images = malignant_images.astype(dtype = np.float32)\n",
        "benign_images /= 255\n",
        "normal_images /= 255\n",
        "malignant_images /= 255\n",
        "\n",
        "for img in benign_images:\n",
        "    X.append(img)\n",
        "for img in normal_images:\n",
        "    X.append(img)\n",
        "for img in malignant_images:\n",
        "    X.append(img)\n",
        "X = np.array(X, dtype=np.float32)\n",
        "\n",
        "for img in benign_masks:\n",
        "    y_segmentation.append(img)\n",
        "for img in normal_masks:\n",
        "    y_segmentation.append(img)\n",
        "for img in malignant_masks:\n",
        "    y_segmentation.append(img)\n",
        "y_segmentation = np.array(y_segmentation, dtype=np.float32)\n",
        "\n",
        "num_samples = len(y_segmentation)\n",
        "perm = np.random.permutation(num_samples)\n",
        "\n",
        "X = X[perm]\n",
        "y_classification = y_classification[perm]\n",
        "y_segmentation = y_segmentation[perm]\n",
        "\n",
        "total_samples = len(X)\n",
        "train_size = int(0.7 * total_samples)\n",
        "val_size = int(0.2 * total_samples)\n",
        "\n",
        "X_train = X[:train_size]\n",
        "y_classification_train = y_classification[:train_size]\n",
        "y_segmentation_train = y_segmentation[:train_size]\n",
        "\n",
        "X_val = X[train_size:train_size + val_size]\n",
        "y_classification_val = y_classification[train_size:train_size + val_size]\n",
        "y_segmentation_val = y_segmentation[train_size:train_size + val_size]\n",
        "\n",
        "X_test = X[train_size + val_size:]\n",
        "y_classification_test = y_classification[train_size + val_size:]\n",
        "y_segmentation_test = y_segmentation[train_size + val_size:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(benign_images) + len(benign_masks) + len(normal_images) + len(normal_masks) + len(malignant_images) + len(malignant_masks))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cje9eKh2RD1",
        "outputId": "92893638-65ec-43eb-93f4-17929bfcf640"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1560\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(benign_images) + len(benign_masks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDnBMlgx2mgt",
        "outputId": "af25b412-9980-456d-f3d9-da84799bbe92"
      },
      "execution_count": 173,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "874"
            ]
          },
          "metadata": {},
          "execution_count": 173
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(normal_images) + len(normal_masks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPwUORR22p6O",
        "outputId": "a73eab4f-f426-4922-ff8c-2384d7162f4c"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "266"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(malignant_images) + len(malignant_masks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NC4Uhei525mA",
        "outputId": "a4daab18-cd85-4e52-b366-e60d2512d08b"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "420"
            ]
          },
          "metadata": {},
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 176,
      "metadata": {
        "id": "K8DfNjJOqShX"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y1, y2):\n",
        "        self.X = X\n",
        "        self.y1 = y1\n",
        "        self.y2 = y2\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y1[idx], self.y2[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 177,
      "metadata": {
        "id": "DvDCSqxwqShY"
      },
      "outputs": [],
      "source": [
        "X_train = torch.Tensor(X_train).to(device)\n",
        "y_classification_train = torch.Tensor(y_classification_train).to(device)\n",
        "y_segmentation_train = torch.Tensor(y_segmentation_train).to(device)\n",
        "\n",
        "X_val = torch.Tensor(X_val).to(device)\n",
        "y_classification_val = torch.Tensor(y_classification_val).to(device)\n",
        "y_segmentation_val = torch.Tensor(y_segmentation_val).to(device)\n",
        "\n",
        "X_test = torch.Tensor(X_test).to(device)\n",
        "y_classification_test = torch.Tensor(y_classification_test).to(device)\n",
        "y_segmentation_test = torch.Tensor(y_segmentation_test).to(device)\n",
        "\n",
        "train_dataset = CustomDataset(X_train, y_classification_train, y_segmentation_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "val_dataset = CustomDataset(X_val, y_classification_val, y_segmentation_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_dataset = CustomDataset(X_test, y_classification_test, y_segmentation_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 178,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4gCqwVWgqShY",
        "outputId": "d4329363-9cdb-4fa0-a4ba-649d37523c38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(780, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ],
      "source": [
        "y_segmentation_train.shape\n",
        "y_classification.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhCfmf6EqShZ",
        "outputId": "154553d0-8c7c-4838-fc04-395b48cdef06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Classification Training Loss: 1.0006626554897853, Train accuracy: 0.5464285714285714, Val Accuracy: 0.5666666686534881, Test Accuracy: 0.575 Segmentation Training Loss: 0.2504407844373158, Classification Validation Loss: 0.9695491552352905, Segmentation Validation Loss: 0.24965447336435317, Classification Test Loss: 0.9611257910728455, Segmentation Test Loss: 0.24973362386226655\n",
            "Epoch: 1, Classification Training Loss: 0.9695791619164603, Train accuracy: 0.5607142857142857, Val Accuracy: 0.5729166686534881, Test Accuracy: 0.575 Segmentation Training Loss: 0.2498514030660902, Classification Validation Loss: 0.9568332552909851, Segmentation Validation Loss: 0.3273313343524933, Classification Test Loss: 0.9443835616111755, Segmentation Test Loss: 0.3297478437423706\n",
            "Epoch: 2, Classification Training Loss: 0.9191389237131391, Train accuracy: 0.6303571428571428, Val Accuracy: 0.6104166686534882, Test Accuracy: 0.6267857193946839 Segmentation Training Loss: 0.254283823285784, Classification Validation Loss: 0.9275634169578553, Segmentation Validation Loss: 0.2659479081630707, Classification Test Loss: 0.914999783039093, Segmentation Test Loss: 0.26325414180755613\n",
            "Epoch: 3, Classification Training Loss: 0.8732900040490287, Train accuracy: 0.6660714285714285, Val Accuracy: 0.6166666686534882, Test Accuracy: 0.6142857193946838 Segmentation Training Loss: 0.2566003607852118, Classification Validation Loss: 0.9211688816547394, Segmentation Validation Loss: 0.27450843155384064, Classification Test Loss: 0.912114143371582, Segmentation Test Loss: 0.27342787981033323\n",
            "Epoch: 4, Classification Training Loss: 0.8509971499443054, Train accuracy: 0.6892857142857143, Val Accuracy: 0.6354166686534881, Test Accuracy: 0.6517857193946839 Segmentation Training Loss: 0.2569223152739661, Classification Validation Loss: 0.9089090704917908, Segmentation Validation Loss: 0.2703155964612961, Classification Test Loss: 0.8860003232955933, Segmentation Test Loss: 0.26654236316680907\n",
            "Epoch: 5, Classification Training Loss: 0.8394725084304809, Train accuracy: 0.7017857142857142, Val Accuracy: 0.65625, Test Accuracy: 0.7178571462631226 Segmentation Training Loss: 0.25729026794433596, Classification Validation Loss: 0.8929666996002197, Segmentation Validation Loss: 0.25255872309207916, Classification Test Loss: 0.8271795153617859, Segmentation Test Loss: 0.251721465587616\n",
            "Epoch: 6, Classification Training Loss: 0.8094314234597343, Train accuracy: 0.7428571428571429, Val Accuracy: 0.6, Test Accuracy: 0.7196428656578064 Segmentation Training Loss: 0.2561990329197475, Classification Validation Loss: 0.9139458417892456, Segmentation Validation Loss: 0.25136529505252836, Classification Test Loss: 0.8222081065177917, Segmentation Test Loss: 0.2513886749744415\n",
            "Epoch: 7, Classification Training Loss: 0.7944577830178398, Train accuracy: 0.7517857142857143, Val Accuracy: 0.6666666686534881, Test Accuracy: 0.7053571462631225 Segmentation Training Loss: 0.25423891629491535, Classification Validation Loss: 0.8769248962402344, Segmentation Validation Loss: 0.2532003134489059, Classification Test Loss: 0.8232179284095764, Segmentation Test Loss: 0.25307486653327943\n",
            "Epoch: 8, Classification Training Loss: 0.780257683140891, Train accuracy: 0.7642857142857142, Val Accuracy: 0.6625, Test Accuracy: 0.7714285850524902 Segmentation Training Loss: 0.2559103923184531, Classification Validation Loss: 0.8672599613666534, Segmentation Validation Loss: 0.2606723517179489, Classification Test Loss: 0.766575288772583, Segmentation Test Loss: 0.2596994638442993\n",
            "Epoch: 9, Classification Training Loss: 0.7905293992587499, Train accuracy: 0.7553571428571428, Val Accuracy: 0.64375, Test Accuracy: 0.7178571462631226 Segmentation Training Loss: 0.2577228018215724, Classification Validation Loss: 0.899580591917038, Segmentation Validation Loss: 0.2647427022457123, Classification Test Loss: 0.8349794626235962, Segmentation Test Loss: 0.26302430033683777\n",
            "Epoch: 10, Classification Training Loss: 0.7947375740323748, Train accuracy: 0.7464285714285714, Val Accuracy: 0.6604166686534881, Test Accuracy: 0.7589285850524903 Segmentation Training Loss: 0.2567428648471832, Classification Validation Loss: 0.8694572687149048, Segmentation Validation Loss: 0.2553191989660263, Classification Test Loss: 0.7844557166099548, Segmentation Test Loss: 0.25606825947761536\n",
            "Epoch: 11, Classification Training Loss: 0.7654425757271903, Train accuracy: 0.7857142857142857, Val Accuracy: 0.7, Test Accuracy: 0.7696428656578064 Segmentation Training Loss: 0.25632492303848264, Classification Validation Loss: 0.8479797124862671, Segmentation Validation Loss: 0.2606896787881851, Classification Test Loss: 0.7853491544723511, Segmentation Test Loss: 0.26145775318145753\n",
            "Epoch: 12, Classification Training Loss: 0.7816757542746408, Train accuracy: 0.7607142857142857, Val Accuracy: 0.6541666686534882, Test Accuracy: 0.6678571462631225 Segmentation Training Loss: 0.2557973214558193, Classification Validation Loss: 0.8886903345584869, Segmentation Validation Loss: 0.26376854479312895, Classification Test Loss: 0.8729119300842285, Segmentation Test Loss: 0.2653615057468414\n",
            "Epoch: 13, Classification Training Loss: 0.7483898537499564, Train accuracy: 0.7982142857142858, Val Accuracy: 0.7, Test Accuracy: 0.7339285850524903 Segmentation Training Loss: 0.2567407786846161, Classification Validation Loss: 0.8403820216655731, Segmentation Validation Loss: 0.2630182236433029, Classification Test Loss: 0.8067582726478577, Segmentation Test Loss: 0.2628689885139465\n",
            "Epoch: 14, Classification Training Loss: 0.7197184886251177, Train accuracy: 0.8339285714285715, Val Accuracy: 0.6979166686534881, Test Accuracy: 0.8089285850524902 Segmentation Training Loss: 0.2567266685622079, Classification Validation Loss: 0.8312684237957001, Segmentation Validation Loss: 0.2552378594875336, Classification Test Loss: 0.746539306640625, Segmentation Test Loss: 0.25520092248916626\n",
            "Epoch: 15, Classification Training Loss: 0.7135993668011257, Train accuracy: 0.8357142857142857, Val Accuracy: 0.6708333373069764, Test Accuracy: 0.6785714387893677 Segmentation Training Loss: 0.2575925784451621, Classification Validation Loss: 0.8714740753173829, Segmentation Validation Loss: 0.2667845249176025, Classification Test Loss: 0.8457376956939697, Segmentation Test Loss: 0.26758098006248476\n",
            "Epoch: 16, Classification Training Loss: 0.7171601142202105, Train accuracy: 0.8214285714285714, Val Accuracy: 0.7145833373069763, Test Accuracy: 0.8357142925262451 Segmentation Training Loss: 0.2573598001684461, Classification Validation Loss: 0.8236848533153533, Segmentation Validation Loss: 0.25917530357837676, Classification Test Loss: 0.7284186244010925, Segmentation Test Loss: 0.25885033011436465\n",
            "Epoch: 17, Classification Training Loss: 0.679768283026559, Train accuracy: 0.8821428571428571, Val Accuracy: 0.6333333343267441, Test Accuracy: 0.7035714387893677 Segmentation Training Loss: 0.2556375903742654, Classification Validation Loss: 0.8953356444835663, Segmentation Validation Loss: 0.2531551688909531, Classification Test Loss: 0.8311622858047485, Segmentation Test Loss: 0.2532580614089966\n",
            "Epoch: 18, Classification Training Loss: 0.654541005407061, Train accuracy: 0.9, Val Accuracy: 0.7041666686534882, Test Accuracy: 0.7428571462631226 Segmentation Training Loss: 0.25541167429515294, Classification Validation Loss: 0.8318242788314819, Segmentation Validation Loss: 0.26138279438018797, Classification Test Loss: 0.8061915755271911, Segmentation Test Loss: 0.2608292520046234\n",
            "Epoch: 19, Classification Training Loss: 0.6429042356354849, Train accuracy: 0.9178571428571428, Val Accuracy: 0.6770833343267441, Test Accuracy: 0.8071428656578064 Segmentation Training Loss: 0.2560274728706905, Classification Validation Loss: 0.8518847763538361, Segmentation Validation Loss: 0.2558066338300705, Classification Test Loss: 0.7532782554626465, Segmentation Test Loss: 0.2559040427207947\n",
            "Epoch: 20, Classification Training Loss: 0.6274625250271388, Train accuracy: 0.9267857142857143, Val Accuracy: 0.6729166686534882, Test Accuracy: 0.6535714387893676 Segmentation Training Loss: 0.25680895958627975, Classification Validation Loss: 0.8576101183891296, Segmentation Validation Loss: 0.2616087138652802, Classification Test Loss: 0.8822648644447326, Segmentation Test Loss: 0.2618231177330017\n",
            "Epoch: 21, Classification Training Loss: 0.6478400758334568, Train accuracy: 0.9071428571428571, Val Accuracy: 0.6979166686534881, Test Accuracy: 0.8214285850524903 Segmentation Training Loss: 0.257871630362102, Classification Validation Loss: 0.8475473701953888, Segmentation Validation Loss: 0.2563034623861313, Classification Test Loss: 0.7356157183647156, Segmentation Test Loss: 0.2557557225227356\n",
            "Epoch: 22, Classification Training Loss: 0.6347581454685756, Train accuracy: 0.9142857142857143, Val Accuracy: 0.73125, Test Accuracy: 0.7821428656578064 Segmentation Training Loss: 0.2578501156398228, Classification Validation Loss: 0.8167953431606293, Segmentation Validation Loss: 0.2591740757226944, Classification Test Loss: 0.7463600277900696, Segmentation Test Loss: 0.2592776775360107\n",
            "Epoch: 23, Classification Training Loss: 0.6267443588801793, Train accuracy: 0.9178571428571428, Val Accuracy: 0.69375, Test Accuracy: 0.7428571462631226 Segmentation Training Loss: 0.2581097534724644, Classification Validation Loss: 0.8559680581092834, Segmentation Validation Loss: 0.2543469160795212, Classification Test Loss: 0.8024580717086792, Segmentation Test Loss: 0.25368621945381165\n",
            "Epoch: 24, Classification Training Loss: 0.6434438790593828, Train accuracy: 0.9089285714285714, Val Accuracy: 0.7229166686534881, Test Accuracy: 0.7464285850524902 Segmentation Training Loss: 0.2559703929083688, Classification Validation Loss: 0.8225080668926239, Segmentation Validation Loss: 0.2582854390144348, Classification Test Loss: 0.7861925363540649, Segmentation Test Loss: 0.25750584006309507\n",
            "Epoch: 25, Classification Training Loss: 0.610297463621412, Train accuracy: 0.9464285714285714, Val Accuracy: 0.7020833373069764, Test Accuracy: 0.7303571462631225 Segmentation Training Loss: 0.25609181097575595, Classification Validation Loss: 0.8441137433052063, Segmentation Validation Loss: 0.2574033349752426, Classification Test Loss: 0.8173360109329224, Segmentation Test Loss: 0.2578161060810089\n",
            "Epoch: 26, Classification Training Loss: 0.6100483519690377, Train accuracy: 0.9446428571428571, Val Accuracy: 0.74375, Test Accuracy: 0.8482142925262451 Segmentation Training Loss: 0.2571835287979671, Classification Validation Loss: 0.8019721269607544, Segmentation Validation Loss: 0.2574971169233322, Classification Test Loss: 0.7036951184272766, Segmentation Test Loss: 0.2573598980903625\n",
            "Epoch: 27, Classification Training Loss: 0.5972034539495196, Train accuracy: 0.9589285714285715, Val Accuracy: 0.6979166686534881, Test Accuracy: 0.7142857193946839 Segmentation Training Loss: 0.2561818378312247, Classification Validation Loss: 0.8412079751491547, Segmentation Validation Loss: 0.2560754895210266, Classification Test Loss: 0.8015068531036377, Segmentation Test Loss: 0.2557117402553558\n",
            "Epoch: 28, Classification Training Loss: 0.5980243904250009, Train accuracy: 0.9535714285714286, Val Accuracy: 0.71875, Test Accuracy: 0.7964285850524903 Segmentation Training Loss: 0.25639369998659406, Classification Validation Loss: 0.8211511969566345, Segmentation Validation Loss: 0.2580565810203552, Classification Test Loss: 0.7395309209823608, Segmentation Test Loss: 0.25808351039886473\n",
            "Epoch: 29, Classification Training Loss: 0.5981461661202567, Train accuracy: 0.9517857142857142, Val Accuracy: 0.71875, Test Accuracy: 0.8089285850524902 Segmentation Training Loss: 0.2565897694655827, Classification Validation Loss: 0.8185361981391907, Segmentation Validation Loss: 0.25944277346134187, Classification Test Loss: 0.7180241823196412, Segmentation Test Loss: 0.25976648926734924\n",
            "Epoch: 30, Classification Training Loss: 0.5913536190986634, Train accuracy: 0.9625, Val Accuracy: 0.71875, Test Accuracy: 0.8321428656578064 Segmentation Training Loss: 0.25569927777562823, Classification Validation Loss: 0.8269457817077637, Segmentation Validation Loss: 0.25612735450267793, Classification Test Loss: 0.7113814949989319, Segmentation Test Loss: 0.2563949406147003\n",
            "Epoch: 31, Classification Training Loss: 0.5819062045642308, Train accuracy: 0.9714285714285714, Val Accuracy: 0.7270833373069763, Test Accuracy: 0.8214285850524903 Segmentation Training Loss: 0.25573021684374125, Classification Validation Loss: 0.8186176538467407, Segmentation Validation Loss: 0.25657024383544924, Classification Test Loss: 0.7302509903907776, Segmentation Test Loss: 0.2568783700466156\n",
            "Epoch: 32, Classification Training Loss: 0.5886454803603036, Train accuracy: 0.9625, Val Accuracy: 0.6416666686534882, Test Accuracy: 0.7446428656578064 Segmentation Training Loss: 0.256024569272995, Classification Validation Loss: 0.896542888879776, Segmentation Validation Loss: 0.25475974977016447, Classification Test Loss: 0.8057364106178284, Segmentation Test Loss: 0.2544576644897461\n",
            "Epoch: 33, Classification Training Loss: 0.5933556250163488, Train accuracy: 0.9571428571428572, Val Accuracy: 0.6729166686534882, Test Accuracy: 0.7428571462631226 Segmentation Training Loss: 0.2562859662941524, Classification Validation Loss: 0.8518858909606933, Segmentation Validation Loss: 0.25454980731010435, Classification Test Loss: 0.7973452568054199, Segmentation Test Loss: 0.2541726529598236\n",
            "Epoch: 34, Classification Training Loss: 0.5909183587346758, Train accuracy: 0.9607142857142857, Val Accuracy: 0.7416666686534882, Test Accuracy: 0.7589285850524903 Segmentation Training Loss: 0.2553606927394867, Classification Validation Loss: 0.8116310954093933, Segmentation Validation Loss: 0.25982317626476287, Classification Test Loss: 0.7954843521118165, Segmentation Test Loss: 0.2602385938167572\n",
            "Epoch: 35, Classification Training Loss: 0.5919745343072074, Train accuracy: 0.9607142857142857, Val Accuracy: 0.7104166686534882, Test Accuracy: 0.7946428656578064 Segmentation Training Loss: 0.2552926940577371, Classification Validation Loss: 0.8230841159820557, Segmentation Validation Loss: 0.2544142514467239, Classification Test Loss: 0.7331166625022888, Segmentation Test Loss: 0.25453934669494627\n",
            "Epoch: 36, Classification Training Loss: 0.5860490781920297, Train accuracy: 0.9696428571428571, Val Accuracy: 0.7229166686534881, Test Accuracy: 0.7785714387893676 Segmentation Training Loss: 0.2545955104487283, Classification Validation Loss: 0.8122772097587585, Segmentation Validation Loss: 0.25771812796592714, Classification Test Loss: 0.7682502865791321, Segmentation Test Loss: 0.25792924165725706\n",
            "Epoch: 37, Classification Training Loss: 0.5893972975867136, Train accuracy: 0.9642857142857143, Val Accuracy: 0.7, Test Accuracy: 0.8089285850524902 Segmentation Training Loss: 0.2554029481751578, Classification Validation Loss: 0.8416196882724762, Segmentation Validation Loss: 0.25459950864315034, Classification Test Loss: 0.7463443040847778, Segmentation Test Loss: 0.2545470595359802\n",
            "Epoch: 38, Classification Training Loss: 0.584546182836805, Train accuracy: 0.9678571428571429, Val Accuracy: 0.7270833373069763, Test Accuracy: 0.8089285850524902 Segmentation Training Loss: 0.25499761019434247, Classification Validation Loss: 0.8263981103897095, Segmentation Validation Loss: 0.2592163860797882, Classification Test Loss: 0.7396920561790467, Segmentation Test Loss: 0.2590651273727417\n",
            "Epoch: 39, Classification Training Loss: 0.585596341746194, Train accuracy: 0.9696428571428571, Val Accuracy: 0.7166666686534882, Test Accuracy: 0.7535714387893677 Segmentation Training Loss: 0.25515186616352625, Classification Validation Loss: 0.8236138164997101, Segmentation Validation Loss: 0.2543996721506119, Classification Test Loss: 0.7973034858703614, Segmentation Test Loss: 0.2534728467464447\n",
            "Epoch: 40, Classification Training Loss: 0.5870139258248466, Train accuracy: 0.9625, Val Accuracy: 0.7416666686534882, Test Accuracy: 0.8089285850524902 Segmentation Training Loss: 0.25551680241312297, Classification Validation Loss: 0.7966200709342957, Segmentation Validation Loss: 0.25584401190280914, Classification Test Loss: 0.7333780765533447, Segmentation Test Loss: 0.2552516579627991\n",
            "Epoch: 41, Classification Training Loss: 0.5973523259162903, Train accuracy: 0.9535714285714286, Val Accuracy: 0.70625, Test Accuracy: 0.7571428656578064 Segmentation Training Loss: 0.2550495173249926, Classification Validation Loss: 0.825835007429123, Segmentation Validation Loss: 0.2562828689813614, Classification Test Loss: 0.7772189736366272, Segmentation Test Loss: 0.25684797763824463\n",
            "Epoch: 42, Classification Training Loss: 0.6072345069476537, Train accuracy: 0.9446428571428571, Val Accuracy: 0.7333333373069764, Test Accuracy: 0.7946428656578064 Segmentation Training Loss: 0.255011670930045, Classification Validation Loss: 0.8177435159683227, Segmentation Validation Loss: 0.2565176784992218, Classification Test Loss: 0.7491064310073853, Segmentation Test Loss: 0.25663713812828065\n",
            "Epoch: 43, Classification Training Loss: 0.5966455936431885, Train accuracy: 0.9571428571428572, Val Accuracy: 0.7375, Test Accuracy: 0.8071428656578064 Segmentation Training Loss: 0.2542880892753601, Classification Validation Loss: 0.8131441473960876, Segmentation Validation Loss: 0.2554313182830811, Classification Test Loss: 0.7256097435951233, Segmentation Test Loss: 0.2553072154521942\n",
            "Epoch: 44, Classification Training Loss: 0.5867392471858434, Train accuracy: 0.9642857142857143, Val Accuracy: 0.6875, Test Accuracy: 0.8071428656578064 Segmentation Training Loss: 0.25394403082983835, Classification Validation Loss: 0.8448718070983887, Segmentation Validation Loss: 0.25277248620986936, Classification Test Loss: 0.7406736731529235, Segmentation Test Loss: 0.25238747596740724\n",
            "Epoch: 45, Classification Training Loss: 0.5815209473882402, Train accuracy: 0.9696428571428571, Val Accuracy: 0.75, Test Accuracy: 0.7946428656578064 Segmentation Training Loss: 0.2535112483160836, Classification Validation Loss: 0.8021888017654419, Segmentation Validation Loss: 0.2538352906703949, Classification Test Loss: 0.7437932252883911, Segmentation Test Loss: 0.25396499037742615\n",
            "Epoch: 46, Classification Training Loss: 0.5822804927825928, Train accuracy: 0.9678571428571429, Val Accuracy: 0.7479166686534882, Test Accuracy: 0.8214285850524903 Segmentation Training Loss: 0.2532344698905945, Classification Validation Loss: 0.8081968188285827, Segmentation Validation Loss: 0.2533934205770493, Classification Test Loss: 0.7244849324226379, Segmentation Test Loss: 0.2533868491649628\n",
            "Epoch: 47, Classification Training Loss: 0.5833273598126003, Train accuracy: 0.9678571428571429, Val Accuracy: 0.6833333373069763, Test Accuracy: 0.7428571462631226 Segmentation Training Loss: 0.25286303077425276, Classification Validation Loss: 0.8530911803245544, Segmentation Validation Loss: 0.252536478638649, Classification Test Loss: 0.7936522006988526, Segmentation Test Loss: 0.2525562822818756\n",
            "Epoch: 48, Classification Training Loss: 0.5819771664483206, Train accuracy: 0.9714285714285714, Val Accuracy: 0.6916666686534881, Test Accuracy: 0.8214285850524903 Segmentation Training Loss: 0.2533354503767831, Classification Validation Loss: 0.8497791230678559, Segmentation Validation Loss: 0.25313196182250974, Classification Test Loss: 0.723182225227356, Segmentation Test Loss: 0.2529156804084778\n",
            "Epoch: 49, Classification Training Loss: 0.5860093780926295, Train accuracy: 0.9642857142857143, Val Accuracy: 0.6833333373069763, Test Accuracy: 0.8089285850524902 Segmentation Training Loss: 0.2536669296877725, Classification Validation Loss: 0.8562841057777405, Segmentation Validation Loss: 0.25295439958572385, Classification Test Loss: 0.7372846245765686, Segmentation Test Loss: 0.252802711725235\n"
          ]
        }
      ],
      "source": [
        "unet = UNet(model).to(device)\n",
        "unet.train()\n",
        "\n",
        "optimizer = Adam(unet.parameters(), lr = 1e-4)\n",
        "segmentation_loss = torch.nn.MSELoss()\n",
        "classification_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "classification_loss_train = []\n",
        "segmentation_loss_train = []\n",
        "classification_loss_val = []\n",
        "segmentation_loss_val = []\n",
        "classification_loss_test = []\n",
        "segmentation_loss_test = []\n",
        "accuracy_list = []\n",
        "val_accuracy_list = []\n",
        "test_accuracy_list = []\n",
        "\n",
        "for i in range(NUM_EPOCHS):\n",
        "    unet.train()\n",
        "    train_loss_class = []\n",
        "    train_loss_seg = []\n",
        "    val_loss_class = []\n",
        "    val_loss_seg = []\n",
        "    test_loss_class = []\n",
        "    test_loss_seg = []\n",
        "    accuracy = []\n",
        "    val_accuracy = []\n",
        "    test_accuracy = []\n",
        "\n",
        "    for batch in train_loader:\n",
        "        X_data, y_class, y_seg = batch\n",
        "        optimizer.zero_grad()\n",
        "        segmentation_output, classification_output = unet(X_data)\n",
        "        segmentation_output = torch.sigmoid(segmentation_output)\n",
        "        classification_output = torch.softmax(classification_output, dim=1)\n",
        "        seg_loss = segmentation_loss(segmentation_output, y_seg)\n",
        "        class_loss = classification_loss(classification_output, y_class)\n",
        "        loss = seg_loss + class_loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_class.append(class_loss.item())\n",
        "        train_loss_seg.append(seg_loss.item())\n",
        "        predicted_classes = torch.argmax(classification_output, dim=1)\n",
        "        temp_class = torch.argmax(y_class, dim=1)\n",
        "        accuracy.append((predicted_classes == temp_class).float().mean().item())\n",
        "\n",
        "    accuracy = sum(accuracy)/len(accuracy)\n",
        "    accuracy_list.append(accuracy)\n",
        "    classification_loss_train.append(sum(train_loss_class) / len(train_loss_class))\n",
        "    segmentation_loss_train.append(sum(train_loss_seg) / len(train_loss_seg))\n",
        "\n",
        "    unet.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss_class = []\n",
        "        val_loss_seg = []\n",
        "        for batch in val_loader:\n",
        "            X_val, y_class_val, y_seg_val = batch\n",
        "            segmentation_output_val, classification_output_val = unet(X_val)\n",
        "            segmentation_output_val = torch.sigmoid(segmentation_output_val)\n",
        "            classification_output_val = torch.softmax(classification_output_val, dim=1)\n",
        "            seg_loss_val = segmentation_loss(segmentation_output_val, y_seg_val)\n",
        "            class_loss_val = classification_loss(classification_output_val, y_class_val)\n",
        "            val_loss_class.append(class_loss_val.item())\n",
        "            val_loss_seg.append(seg_loss_val.item())\n",
        "            predicted_classes = torch.argmax(classification_output_val, dim=1)\n",
        "            temp_class = torch.argmax(y_class_val, dim=1)\n",
        "            val_accuracy.append((predicted_classes == temp_class).float().mean().item())\n",
        "\n",
        "        val_accuracy = sum(val_accuracy)/len(val_accuracy)\n",
        "        val_accuracy_list.append(val_accuracy)\n",
        "        classification_loss_val.append(sum(val_loss_class) / len(val_loss_class))\n",
        "        segmentation_loss_val.append(sum(val_loss_seg) / len(val_loss_seg))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        test_loss_class = []\n",
        "        test_loss_seg = []\n",
        "        for batch in test_loader:\n",
        "            X_test, y_class_test, y_seg_test = batch\n",
        "            segmentation_output_test, classification_output_test = unet(X_test)\n",
        "            segmentation_output_test = torch.sigmoid(segmentation_output_test)\n",
        "            classification_output_test = torch.softmax(classification_output_test, dim=1)\n",
        "            seg_loss_test = segmentation_loss(segmentation_output_test, y_seg_test)\n",
        "            class_loss_test = classification_loss(classification_output_test, y_class_test)\n",
        "            test_loss_class.append(class_loss_test.item())\n",
        "            test_loss_seg.append(seg_loss_test.item())\n",
        "            predicted_classes = torch.argmax(classification_output_test, dim=1)\n",
        "            temp_class = torch.argmax(y_class_test, dim=1)\n",
        "            test_accuracy.append((predicted_classes == temp_class).float().mean().item())\n",
        "\n",
        "        test_accuracy = sum(test_accuracy)/len(test_accuracy)\n",
        "        test_accuracy_list.append(test_accuracy)\n",
        "        classification_loss_test.append(sum(test_loss_class) / len(test_loss_class))\n",
        "        segmentation_loss_test.append(sum(test_loss_seg) / len(test_loss_seg))\n",
        "\n",
        "    print(f\"Epoch: {i}, Classification Training Loss: {classification_loss_train[-1]}, Train accuracy: {accuracy_list[-1]}, Val Accuracy: {val_accuracy_list[-1]}, Test Accuracy: {test_accuracy_list[-1]} Segmentation Training Loss: {segmentation_loss_train[-1]}, Classification Validation Loss: {classification_loss_val[-1]}, Segmentation Validation Loss: {segmentation_loss_val[-1]}, Classification Test Loss: {classification_loss_test[-1]}, Segmentation Test Loss: {segmentation_loss_test[-1]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(unet.state_dict(), 'unet1.pth')"
      ],
      "metadata": {
        "id": "1ZC4RO27Asom"
      },
      "execution_count": 180,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lists_to_save = {\n",
        "    'classification_loss_train': classification_loss_train,\n",
        "    'segmentation_loss_train': segmentation_loss_train,\n",
        "    'classification_loss_val': classification_loss_val,\n",
        "    'segmentation_loss_val': segmentation_loss_val,\n",
        "    'classification_loss_test': classification_loss_test,\n",
        "    'segmentation_loss_test': segmentation_loss_test,\n",
        "    'accuracy_list': accuracy_list,\n",
        "    'val_accuracy_list': val_accuracy_list,\n",
        "    'test_accuracy_list': test_accuracy_list\n",
        "}\n",
        "\n",
        "with open('unet_results_1.pkl', 'wb') as f:\n",
        "    pickle.dump(lists_to_save, f)"
      ],
      "metadata": {
        "id": "S1LNQFFsAs-l"
      },
      "execution_count": 181,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}