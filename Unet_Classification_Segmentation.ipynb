{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW, Adam\n",
    "from utils.utils import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\mateu/.cache\\torch\\hub\\mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "    in_channels=3, out_channels=1, init_features=32, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(UNet, self).__init__()\n",
    "        self.model = base_model\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=3)\n",
    "        self.fc1 = nn.Linear(28900, 512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.output = nn.Linear(128, 3) \n",
    "\n",
    "    def forward(self, x):\n",
    "        segmented = self.model(x)\n",
    "        concatenated = torch.cat((x, segmented), dim=1)\n",
    "        classified = self.maxpool(concatenated)\n",
    "        classified = classified.view(classified.size(0), -1)\n",
    "        classified = self.fc1(classified)\n",
    "        classified = self.relu1(classified)\n",
    "        classified = self.fc2(classified)\n",
    "        classified = self.relu2(classified)\n",
    "        classified = self.fc3(classified)\n",
    "        classified = self.relu3(classified)\n",
    "        output = self.output(classified)\n",
    "        return segmented, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (model): UNet(\n",
      "    (encoder1): Sequential(\n",
      "      (enc1conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (enc1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (enc1relu1): ReLU(inplace=True)\n",
      "      (enc1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (enc1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (enc1relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (encoder2): Sequential(\n",
      "      (enc2conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (enc2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (enc2relu1): ReLU(inplace=True)\n",
      "      (enc2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (enc2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (enc2relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (encoder3): Sequential(\n",
      "      (enc3conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (enc3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (enc3relu1): ReLU(inplace=True)\n",
      "      (enc3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (enc3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (enc3relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (encoder4): Sequential(\n",
      "      (enc4conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (enc4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (enc4relu1): ReLU(inplace=True)\n",
      "      (enc4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (enc4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (enc4relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (bottleneck): Sequential(\n",
      "      (bottleneckconv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bottlenecknorm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bottleneckrelu1): ReLU(inplace=True)\n",
      "      (bottleneckconv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bottlenecknorm2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bottleneckrelu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (upconv4): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (decoder4): Sequential(\n",
      "      (dec4conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dec4norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dec4relu1): ReLU(inplace=True)\n",
      "      (dec4conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dec4norm2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dec4relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (upconv3): ConvTranspose2d(256, 128, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (decoder3): Sequential(\n",
      "      (dec3conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dec3norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dec3relu1): ReLU(inplace=True)\n",
      "      (dec3conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dec3norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dec3relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (upconv2): ConvTranspose2d(128, 64, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (decoder2): Sequential(\n",
      "      (dec2conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dec2norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dec2relu1): ReLU(inplace=True)\n",
      "      (dec2conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dec2norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dec2relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (upconv1): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (decoder1): Sequential(\n",
      "      (dec1conv1): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dec1norm1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dec1relu1): ReLU(inplace=True)\n",
      "      (dec1conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (dec1norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (dec1relu2): ReLU(inplace=True)\n",
      "    )\n",
      "    (conv): Conv2d(32, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=28900, out_features=512, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (relu3): ReLU()\n",
      "  (output): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "unet = UNet(model)\n",
    "print(unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters:  22724964\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Example usage:\n",
    "total_params = count_parameters(unet)\n",
    "print(\"Total trainable parameters: \", total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_images, benign_masks = read_data(directory = \"data//benign\", target_size=(256, 256))\n",
    "normal_images, normal_masks = read_data(directory = \"data//normal\", target_size=(256, 256))\n",
    "malignant_images, malignant_masks = read_data(directory = \"data//malignant\", target_size=(256, 256))\n",
    "\n",
    "benign_masks = np.reshape(benign_masks, (benign_masks.shape[0], 1, benign_masks.shape[1], benign_masks.shape[2]))\n",
    "normal_masks = np.reshape(normal_masks, (normal_masks.shape[0], 1, normal_masks.shape[1], normal_masks.shape[2]))\n",
    "malignant_masks = np.reshape(malignant_masks, (malignant_masks.shape[0], 1, malignant_masks.shape[1], malignant_masks.shape[2]))\n",
    "\n",
    "benign_images = np.transpose(benign_images, (0, 3, 1, 2))\n",
    "normal_images = np.transpose(normal_images, (0, 3, 1, 2))\n",
    "malignant_images = np.transpose(malignant_images, (0, 3, 1, 2))\n",
    "\n",
    "benign_values = np.array([1, 0, 0], dtype = np.float32)\n",
    "normal_values = np.array([0, 1, 0], dtype = np.float32)\n",
    "malignant_values = np.array([0, 0, 1], dtype = np.float32)\n",
    "num_benign = len(benign_images)\n",
    "num_normal = len(normal_images)\n",
    "num_malignant = len(malignant_images)\n",
    "y_classification = []\n",
    "y_segmentation = []\n",
    "X = []\n",
    "\n",
    "for i in range(num_benign):\n",
    "    y_classification.append(benign_values)\n",
    "for i in range(num_normal):\n",
    "    y_classification.append(normal_values)\n",
    "for i in range(num_malignant):\n",
    "    y_classification.append(malignant_values)\n",
    "y_classification = np.array(y_classification, dtype = np.float32)\n",
    "\n",
    "benign_masks[benign_masks>0] = 1\n",
    "normal_masks[normal_masks>0] = 1\n",
    "malignant_masks[malignant_masks>0] = 1\n",
    "benign_images = benign_images.astype(dtype = np.float32)\n",
    "normal_images = normal_images.astype(dtype = np.float32)\n",
    "malignant_images = malignant_images.astype(dtype = np.float32)\n",
    "benign_images /= 255\n",
    "normal_images /= 255\n",
    "malignant_images /= 255\n",
    "\n",
    "for img in benign_images:\n",
    "    X.append(img)\n",
    "for img in normal_images:\n",
    "    X.append(img)\n",
    "for img in malignant_images:\n",
    "    X.append(img)\n",
    "X = np.array(X, dtype=np.float32)\n",
    "\n",
    "for img in benign_masks:\n",
    "    y_segmentation.append(img)\n",
    "for img in normal_masks:\n",
    "    y_segmentation.append(img)\n",
    "for img in malignant_masks:\n",
    "    y_segmentation.append(img)\n",
    "y_segmentation = np.array(y_segmentation, dtype=np.float32)\n",
    "\n",
    "num_samples = len(y_segmentation)\n",
    "perm = np.random.permutation(num_samples)\n",
    "\n",
    "X = X[perm]\n",
    "y_classification = y_classification[perm]\n",
    "y_segmentation = y_segmentation[perm]\n",
    "\n",
    "total_samples = len(X)\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.2 * total_samples)\n",
    "\n",
    "X_train = X[:train_size]\n",
    "y_classification_train = y_classification[:train_size]\n",
    "y_segmentation_train = y_segmentation[:train_size]\n",
    "\n",
    "X_val = X[train_size:train_size + val_size]\n",
    "y_classification_val = y_classification[train_size:train_size + val_size]\n",
    "y_segmentation_val = y_segmentation[train_size:train_size + val_size]\n",
    "\n",
    "X_test = X[train_size + val_size:]\n",
    "y_classification_test = y_classification[train_size + val_size:]\n",
    "y_segmentation_test = y_segmentation[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y1, y2):\n",
    "        self.X = X\n",
    "        self.y1 = y1\n",
    "        self.y2 = y2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y1[idx], self.y2[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.Tensor(X_train)\n",
    "y_classification_train = torch.Tensor(y_classification_train)\n",
    "y_segmentation_train = torch.Tensor(y_segmentation_train)\n",
    "\n",
    "X_val = torch.Tensor(X_val)\n",
    "y_classification_val = torch.Tensor(y_classification_val)\n",
    "y_segmentation_val = torch.Tensor(y_segmentation_val)\n",
    "\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_classification_test = torch.Tensor(y_classification_test)\n",
    "y_segmentation_test = torch.Tensor(y_segmentation_test)\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_classification_train, y_segmentation_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_dataset = CustomDataset(X_val, y_classification_val, y_segmentation_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = CustomDataset(X_test, y_classification_test, y_segmentation_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780, 3)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_segmentation_train.shape\n",
    "y_classification.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m classification_loss_train\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(train_loss_class) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loss_class))\n\u001b[0;32m     36\u001b[0m segmentation_loss_train\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28msum\u001b[39m(train_loss_seg) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loss_seg))\n\u001b[1;32m---> 37\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Classification Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassification_loss_train[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Segmentation Training Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegmentation_loss_train[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Classification Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mclassification_loss_val\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Segmentation Validation Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegmentation_loss_val[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Classification Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclassification_loss_test[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Segmentation Test Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msegmentation_loss_test[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "unet = UNet(model)\n",
    "unet.train()\n",
    "\n",
    "optimizer = Adam(unet.parameters(), lr = 1e-4)\n",
    "segmentation_loss = nn.CrossEntropyLoss()\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "classification_loss_train = []\n",
    "segmentation_loss_train = []\n",
    "classification_loss_val = []\n",
    "segmentation_loss_val = []\n",
    "classification_loss_test = []\n",
    "segmentation_loss_test = []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    unet.train()\n",
    "    train_loss_class = []\n",
    "    train_loss_seg = []\n",
    "    val_loss_class = []\n",
    "    val_loss_seg = []\n",
    "    test_loss_class = []\n",
    "    test_loss_seg = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X_data, y_class, y_seg = batch\n",
    "        optimizer.zero_grad()\n",
    "        segmentation_output, classification_output = unet(X_data)\n",
    "        segmentation_output = torch.sigmoid(segmentation_output)\n",
    "        classification_output = torch.softmax(classification_output, dim=1)\n",
    "        seg_loss = segmentation_loss(segmentation_output, y_seg)\n",
    "        class_loss = classification_loss(classification_output, y_class)\n",
    "        loss = seg_loss + class_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_class.append(class_loss.item())\n",
    "        train_loss_seg.append(seg_loss.item())\n",
    "\n",
    "    classification_loss_train.append(sum(train_loss_class) / len(train_loss_class))\n",
    "    segmentation_loss_train.append(sum(train_loss_seg) / len(train_loss_seg))\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_class = []\n",
    "        val_loss_seg = []\n",
    "        for batch in val_loader:\n",
    "            X_val, y_class_val, y_seg_val = batch\n",
    "            segmentation_output_val, classification_output_val = unet(X_val)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_val = segmentation_loss(segmentation_output_val, y_seg_val)\n",
    "            class_loss_val = classification_loss(classification_output_val, y_class_val)\n",
    "            val_loss_class.append(class_loss_val.item())\n",
    "            val_loss_seg.append(seg_loss_val.item())\n",
    "        \n",
    "        classification_loss_val.append(sum(val_loss_class) / len(val_loss_class))\n",
    "        segmentation_loss_val.append(sum(val_loss_seg) / len(val_loss_seg))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss_class = []\n",
    "        test_loss_seg = []\n",
    "        for batch in test_loader:\n",
    "            X_test, y_class_test, y_seg_test = batch\n",
    "            segmentation_output_test, classification_output_test = unet(X_test)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_test = segmentation_loss(segmentation_output_test, y_seg_test)\n",
    "            class_loss_test = classification_loss(classification_output_test, y_class_test)\n",
    "            test_loss_class.append(class_loss_test.item())\n",
    "            test_loss_seg.append(seg_loss_test.item())\n",
    "        \n",
    "        classification_loss_test.append(sum(test_loss_class) / len(test_loss_class))\n",
    "        segmentation_loss_test.append(sum(test_loss_seg) / len(test_loss_seg))\n",
    "\n",
    "    print(f\"Epoch: {i}, Classification Training Loss: {classification_loss_train[-1]}, Segmentation Training Loss: {segmentation_loss_train[-1]}, Classification Validation Loss: {classification_loss_val[-1]}, Segmentation Validation Loss: {segmentation_loss_val[-1]}, Classification Test Loss: {classification_loss_test[-1]}, Segmentation Test Loss: {segmentation_loss_test[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(model)\n",
    "unet.train()\n",
    "\n",
    "optimizer = Adam(unet.parameters(), lr = 1e-4)\n",
    "segmentation_loss = nn.CrossEntropyLoss()\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "classification_loss_train = []\n",
    "segmentation_loss_train = []\n",
    "classification_loss_val = []\n",
    "segmentation_loss_val = []\n",
    "classification_loss_test = []\n",
    "segmentation_loss_test = []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    unet.train()\n",
    "    train_loss_class = []\n",
    "    train_loss_seg = []\n",
    "    val_loss_class = []\n",
    "    val_loss_seg = []\n",
    "    test_loss_class = []\n",
    "    test_loss_seg = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X_data, y_class, y_seg = batch\n",
    "        optimizer.zero_grad()\n",
    "        segmentation_output, classification_output = unet(X_data)\n",
    "        segmentation_output = torch.sigmoid(segmentation_output)\n",
    "        classification_output = torch.softmax(classification_output, dim=1)\n",
    "        seg_loss = segmentation_loss(segmentation_output, y_seg)\n",
    "        class_loss = classification_loss(classification_output, y_class)\n",
    "        loss = seg_loss + class_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_class.append(class_loss.item())\n",
    "        train_loss_seg.append(seg_loss.item())\n",
    "\n",
    "    classification_loss_train.append(sum(train_loss_class) / len(train_loss_class))\n",
    "    segmentation_loss_train.append(sum(train_loss_seg) / len(train_loss_seg))\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_class = []\n",
    "        val_loss_seg = []\n",
    "        for batch in val_loader:\n",
    "            X_val, y_class_val, y_seg_val = batch\n",
    "            segmentation_output_val, classification_output_val = unet(X_val)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_val = segmentation_loss(segmentation_output_val, y_seg_val)\n",
    "            class_loss_val = classification_loss(classification_output_val, y_class_val)\n",
    "            val_loss_class.append(class_loss_val.item())\n",
    "            val_loss_seg.append(seg_loss_val.item())\n",
    "        \n",
    "        classification_loss_val.append(sum(val_loss_class) / len(val_loss_class))\n",
    "        segmentation_loss_val.append(sum(val_loss_seg) / len(val_loss_seg))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss_class = []\n",
    "        test_loss_seg = []\n",
    "        for batch in test_loader:\n",
    "            X_test, y_class_test, y_seg_test = batch\n",
    "            segmentation_output_test, classification_output_test = unet(X_test)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_test = segmentation_loss(segmentation_output_test, y_seg_test)\n",
    "            class_loss_test = classification_loss(classification_output_test, y_class_test)\n",
    "            test_loss_class.append(class_loss_test.item())\n",
    "            test_loss_seg.append(seg_loss_test.item())\n",
    "        \n",
    "        classification_loss_test.append(sum(test_loss_class) / len(test_loss_class))\n",
    "        segmentation_loss_test.append(sum(test_loss_seg) / len(test_loss_seg))\n",
    "\n",
    "    print(f\"Epoch: {i}, Classification Training Loss: {classification_loss_train[-1]}, Segmentation Training Loss: {segmentation_loss_train[-1]}, Classification Validation Loss: {classification_loss_val[-1]}, Segmentation Validation Loss: {segmentation_loss_val[-1]}, Classification Test Loss: {classification_loss_test[-1]}, Segmentation Test Loss: {segmentation_loss_test[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = 0.8\n",
    "b2 = 0.85\n",
    "unet = UNet(model)\n",
    "unet.train()\n",
    "\n",
    "optimizer = Adam(unet.parameters(), lr = 1e-3, betas=(b1, b2), weight_decay=0.0005)\n",
    "segmentation_loss = nn.CrossEntropyLoss()\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "classification_loss_train = []\n",
    "segmentation_loss_train = []\n",
    "classification_loss_val = []\n",
    "segmentation_loss_val = []\n",
    "classification_loss_test = []\n",
    "segmentation_loss_test = []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    unet.train()\n",
    "    train_loss_class = []\n",
    "    train_loss_seg = []\n",
    "    val_loss_class = []\n",
    "    val_loss_seg = []\n",
    "    test_loss_class = []\n",
    "    test_loss_seg = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X_data, y_class, y_seg = batch\n",
    "        optimizer.zero_grad()\n",
    "        segmentation_output, classification_output = unet(X_data)\n",
    "        segmentation_output = torch.sigmoid(segmentation_output)\n",
    "        classification_output = torch.softmax(classification_output, dim=1)\n",
    "        seg_loss = segmentation_loss(segmentation_output, y_seg)\n",
    "        class_loss = classification_loss(classification_output, y_class)\n",
    "        loss = seg_loss + class_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_class.append(class_loss.item())\n",
    "        train_loss_seg.append(seg_loss.item())\n",
    "\n",
    "    classification_loss_train.append(sum(train_loss_class) / len(train_loss_class))\n",
    "    segmentation_loss_train.append(sum(train_loss_seg) / len(train_loss_seg))\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_class = []\n",
    "        val_loss_seg = []\n",
    "        for batch in val_loader:\n",
    "            X_val, y_class_val, y_seg_val = batch\n",
    "            segmentation_output_val, classification_output_val = unet(X_val)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_val = segmentation_loss(segmentation_output_val, y_seg_val)\n",
    "            class_loss_val = classification_loss(classification_output_val, y_class_val)\n",
    "            val_loss_class.append(class_loss_val.item())\n",
    "            val_loss_seg.append(seg_loss_val.item())\n",
    "        \n",
    "        classification_loss_val.append(sum(val_loss_class) / len(val_loss_class))\n",
    "        segmentation_loss_val.append(sum(val_loss_seg) / len(val_loss_seg))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss_class = []\n",
    "        test_loss_seg = []\n",
    "        for batch in test_loader:\n",
    "            X_test, y_class_test, y_seg_test = batch\n",
    "            segmentation_output_test, classification_output_test = unet(X_test)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_test = segmentation_loss(segmentation_output_test, y_seg_test)\n",
    "            class_loss_test = classification_loss(classification_output_test, y_class_test)\n",
    "            test_loss_class.append(class_loss_test.item())\n",
    "            test_loss_seg.append(seg_loss_test.item())\n",
    "        \n",
    "        classification_loss_test.append(sum(test_loss_class) / len(test_loss_class))\n",
    "        segmentation_loss_test.append(sum(test_loss_seg) / len(test_loss_seg))\n",
    "\n",
    "    print(f\"Epoch: {i}, Classification Training Loss: {classification_loss_train[-1]}, Segmentation Training Loss: {segmentation_loss_train[-1]}, Classification Validation Loss: {classification_loss_val[-1]}, Segmentation Validation Loss: {segmentation_loss_val[-1]}, Classification Test Loss: {classification_loss_test[-1]}, Segmentation Test Loss: {segmentation_loss_test[-1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = UNet(model)\n",
    "unet.train()\n",
    "\n",
    "optimizer = AdamW(unet.parameters(), lr = 1e-4)\n",
    "segmentation_loss = nn.CrossEntropyLoss()\n",
    "classification_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "classification_loss_train = []\n",
    "segmentation_loss_train = []\n",
    "classification_loss_val = []\n",
    "segmentation_loss_val = []\n",
    "classification_loss_test = []\n",
    "segmentation_loss_test = []\n",
    "\n",
    "for i in range(NUM_EPOCHS):\n",
    "    unet.train()\n",
    "    train_loss_class = []\n",
    "    train_loss_seg = []\n",
    "    val_loss_class = []\n",
    "    val_loss_seg = []\n",
    "    test_loss_class = []\n",
    "    test_loss_seg = []\n",
    "\n",
    "    for batch in train_loader:\n",
    "        X_data, y_class, y_seg = batch\n",
    "        optimizer.zero_grad()\n",
    "        segmentation_output, classification_output = unet(X_data)\n",
    "        segmentation_output = torch.sigmoid(segmentation_output)\n",
    "        classification_output = torch.softmax(classification_output, dim=1)\n",
    "        seg_loss = segmentation_loss(segmentation_output, y_seg)\n",
    "        class_loss = classification_loss(classification_output, y_class)\n",
    "        loss = seg_loss + class_loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_class.append(class_loss.item())\n",
    "        train_loss_seg.append(seg_loss.item())\n",
    "\n",
    "    classification_loss_train.append(sum(train_loss_class) / len(train_loss_class))\n",
    "    segmentation_loss_train.append(sum(train_loss_seg) / len(train_loss_seg))\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss_class = []\n",
    "        val_loss_seg = []\n",
    "        for batch in val_loader:\n",
    "            X_val, y_class_val, y_seg_val = batch\n",
    "            segmentation_output_val, classification_output_val = unet(X_val)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_val = segmentation_loss(segmentation_output_val, y_seg_val)\n",
    "            class_loss_val = classification_loss(classification_output_val, y_class_val)\n",
    "            val_loss_class.append(class_loss_val.item())\n",
    "            val_loss_seg.append(seg_loss_val.item())\n",
    "        \n",
    "        classification_loss_val.append(sum(val_loss_class) / len(val_loss_class))\n",
    "        segmentation_loss_val.append(sum(val_loss_seg) / len(val_loss_seg))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        test_loss_class = []\n",
    "        test_loss_seg = []\n",
    "        for batch in test_loader:\n",
    "            X_test, y_class_test, y_seg_test = batch\n",
    "            segmentation_output_test, classification_output_test = unet(X_test)\n",
    "            segmentation_output = torch.sigmoid(segmentation_output)\n",
    "            classification_output = torch.softmax(classification_output, dim=1)\n",
    "            seg_loss_test = segmentation_loss(segmentation_output_test, y_seg_test)\n",
    "            class_loss_test = classification_loss(classification_output_test, y_class_test)\n",
    "            test_loss_class.append(class_loss_test.item())\n",
    "            test_loss_seg.append(seg_loss_test.item())\n",
    "        \n",
    "        classification_loss_test.append(sum(test_loss_class) / len(test_loss_class))\n",
    "        segmentation_loss_test.append(sum(test_loss_seg) / len(test_loss_seg))\n",
    "\n",
    "    print(f\"Epoch: {i}, Classification Training Loss: {classification_loss_train[-1]}, Segmentation Training Loss: {segmentation_loss_train[-1]}, Classification Validation Loss: {classification_loss_val[-1]}, Segmentation Validation Loss: {segmentation_loss_val[-1]}, Classification Test Loss: {classification_loss_test[-1]}, Segmentation Test Loss: {segmentation_loss_test[-1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
